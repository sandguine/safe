#!/usr/bin/env python3
"""
Simple Multi-Model Evaluation Runner
===================================

This script runs the fixed multi-model evaluation with proper error handling.
"""

import asyncio
import os
import sys
from pathlib import Path

# Load environment variables from .env file first
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("âœ… Loaded environment variables from .env file")
except ImportError:
    print("âš ï¸ python-dotenv not installed, using system environment variables")
except Exception as e:
    print(f"âš ï¸ Error loading .env file: {e}")

# Add oversight to path
sys.path.insert(0, str(Path(__file__).parent))

def check_api_key():
    """Check if API key is available."""
    api_key = os.getenv("CLAUDE_API_KEY")
    if not api_key:
        print("âŒ CLAUDE_API_KEY environment variable not found!")
        print("   Please set it with: export CLAUDE_API_KEY='your-api-key-here'")
        print("   Or create a .env file with: CLAUDE_API_KEY=your-key")
        return False

    # Mask API key for display
    masked_key = f"{api_key[:8]}...{api_key[-4:]}" if len(api_key) > 12 else "***"
    print(f"âœ… API key found: {masked_key}")

    # Check format
    if not api_key.startswith("sk-ant-"):
        print("âŒ API key format appears invalid (should start with 'sk-ant-')")
        return False

    print("âœ… API key format appears valid")
    return True

async def run_evaluation():
    """Run the fixed multi-model evaluation."""
    try:
        from fixed_multi_model_evaluation import FixedMultiModelEvaluator

        print("ğŸš€ Starting Fixed Multi-Model Evaluation")
        print("=" * 60)

        # Initialize evaluator
        evaluator = FixedMultiModelEvaluator()
        print(f"âœ… Evaluator initialized with {len(evaluator.models)} models")

        # Run evaluation
        results = await evaluator.run_comprehensive_evaluation()

        print("âœ… Evaluation completed successfully!")
        return results

    except Exception as e:
        print(f"âŒ Evaluation failed: {e}")
        return None

async def main():
    """Main function."""
    print("ğŸ”§ Fixed Multi-Model Evaluation")
    print("=" * 60)

    # Check API key
    if not check_api_key():
        print("\nâŒ Cannot proceed without valid API key")
        sys.exit(1)

    # Clean old results
    old_results_dir = "results/multi_model_evaluation"
    if os.path.exists(old_results_dir):
        print(f"\nğŸ§¹ Removing old results: {old_results_dir}")
        import shutil
        shutil.rmtree(old_results_dir)
        print("âœ… Old results removed")

    # Run evaluation
    print("\nğŸš€ Running evaluation...")
    results = await run_evaluation()

    if results:
        print("\nğŸ‰ EVALUATION COMPLETED SUCCESSFULLY!")
        print("=" * 60)
        print("ğŸ“Š Results saved to: results/fixed_multi_model_evaluation/")
        print("ğŸ“ˆ Charts saved to: results/fixed_multi_model_evaluation/charts/")
        print("ğŸ“‹ Logs saved to: results/fixed_multi_model_evaluation/logs/")
    else:
        print("\nâŒ Evaluation failed!")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
