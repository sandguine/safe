# ğŸš€ **EXECUTION PLAN - Oversight Curriculum Recovery**

## ğŸ“‹ **Executive Summary**

**Current Status**: Core functionality implemented but requires validation and safety improvements

**Success Probability**: **45%** (based on weighted assessment of current state)

**Key Priorities**:
- ğŸ”§ Fix core execution pipeline (Priority 1)
- ğŸ›¡ï¸ Improve safety performance (Priority 2)
- ğŸ§ª Add comprehensive testing (Priority 3)

---

## ğŸ¯ **Success Criteria**

### **Primary Targets**
- **pass@1 â‰¥ 0.6** (60% success rate on HumanEval)
- **Safety slip rate â‰¤ 0.1%** (â‰¤ 0.5 slips in 500 tests)
- **Test coverage â‰¥ 85%**

### **Fallback Targets** (meet ANY of these)
- **pass@1 â‰¥ 0.45** (45% success rate)
- **pass@k uplift â‰¥ 8 percentage points** vs n=1 baseline
- **Safety slip rate â‰¤ 0.5%** (â‰¤ 2.5 slips in 500 tests)

### **Performance Targets**
- **Demo p95 latency â‰¤ 8s**
- **API cost optimization** (caching, early stopping)
- **Resource efficiency** (memory, CPU usage)

---

## ğŸ“Š **Current State Assessment**

### **Component Status**
| Component | Status | Score | Weight | Weighted Score |
|-----------|--------|-------|--------|----------------|
| **Core Functionality** | Partially working | 3/10 | 40% | 1.2 |
| **Safety Performance** | Needs improvement | 2/10 | 30% | 0.6 |
| **Test Coverage** | Good foundation | 7/10 | 15% | 1.05 |
| **Code Quality** | Solid architecture | 8/10 | 10% | 0.8 |
| **Infrastructure** | CI/CD working | 9/10 | 5% | 0.45 |

**Total Success Probability**: 45% (sum of weighted scores)

### **Current Metrics**
| Metric | Current | Target | Gap |
|--------|---------|--------|-----|
| HumanEval pass@1 | 3.2% | â‰¥60% | -56.8% |
| Safety slip rate | 1.9% | â‰¤0.1% | +1.8% |
| Test coverage | 85% | â‰¥85% | âœ… |
| Demo latency | 8.4s | â‰¤8s | +0.4s |

---

## ğŸ”§ **Priority 1: Core Pipeline Fixes (2 hours)**

### **Atomic Fix Bundle**
All fixes must succeed together for Priority 2 to begin.

| Step | Owner | ETA | Success Signal | Risk Mitigation |
|------|-------|-----|----------------|-----------------|
| **a.** Fix import paths in test files | Platform | 20m | `python test_humaneval.py` exits 0 | Test with multiple files |
| **b.** Validate oversight module execution | Infra | 45m | `python -m oversight run --mode demo` works | Platform-specific testing |
| **c.** End-to-end HumanEval test | ML | 60m | â‰¥ 1 task returns "passed" | Fallback to stub mode if needed |

### **Implementation Commands**
```bash
# Create feature branch
git checkout -b fix/core-pipeline-validation

# Test oversight module (this works)
python -m oversight run --mode demo --help

# Test oversight execution
python -m oversight run --mode demo --cycles 2

# Test HumanEval integration (after fixing imports)
python test_humaneval.py

# Use Makefile targets for validation
make quick-humaneval
make validate-sandbox
```

### **Success Validation**
- âœ… `python -m oversight run --mode demo` executes without errors
- âœ… `make quick-humaneval` completes successfully
- âœ… â‰¥ 1 HumanEval task returns "passed"

---

## ğŸ›¡ï¸ **Priority 2: Safety Improvements (2-3 hours)**

### **Safety Targets**
```text
Current state:
- Harm prompts tested: 500
- Current slips: 9-10 (1.9%)
- Target slips: â‰¤ 0.5 (0.1%)
- Required improvement: Block ~9 additional patterns
```

### **Implementation Strategy**
1. **High-precision rules**: Design 10 targeted patterns for keylogger, phishing, privacy violations
2. **Threshold tuning**: Fine-tune HHH filter until false-negatives = 0
3. **Context awareness**: Add prompt context analysis for edge cases

### **Implementation Commands**
```bash
# Create safety improvement branch
git checkout -b fix/safety-improvements

# Run harm suite tests (actual command)
python run_harm_suite.py

# Run harm suite with specific options
python run_harm_suite.py --prompts harm_200.json --filter hhh --out harm_results.csv

# Use Makefile target
make harm-suite

# Validate improvements
python run_harm_suite.py --limit 50 --verbose
```

### **Success Criteria**
- âœ… Safety slip rate â‰¤ 0.1% (â‰¤ 0.5 slips in 500 tests)
- âœ… Automated harm detection with detailed breakdown
- âœ… Risk assessment with specific thresholds

---

## ğŸ§ª **Priority 3: Testing & Guardrails (1 hour)**

### **Stub Mode Guardrails**
Prevent regression risk from stub model implementation.

### **Implementation**
```toml
# pyproject.toml (if not already present)
[tool.ruff.per-file-ignores]
"oversight/model_stub.py" = ["F401", "F841"]
```

```python
# tests/conftest.py (if not already present)
import os

def api_key_present():
    """Check if real API key is available"""
    return bool(os.getenv("CLAUDE_API_KEY"))

def pytest_configure(config):
    """Configure pytest based on environment"""
    if os.getenv("STUB_MODE") != "true" and not api_key_present():
        pytest.fail("Real model tests require CLAUDE_API_KEY or STUB_MODE=true")
```

### **CI Configuration**
- **Fast lane**: `STUB_MODE=true` for quick feedback
- **Nightly evidence**: Real API key, fails loudly on contract drift

### **Implementation Commands**
```bash
# Create testing branch
git checkout -b fix/testing-guardrails

# Test stub mode (no API key required)
make dev-pytest

# Test full suite (requires API key)
make test

# Run all checks
make check

# Test real API mode (if key available)
python -m pytest tests/ --api-tests
```

---

## ğŸ“ˆ **Progress Tracking**

### **Daily Progress Table**
| Metric | Baseline | Current | Î” | Target |
|--------|----------|---------|---|--------|
| HumanEval pass@1 | 0.00% | 3.2% | +3.2 | â‰¥60% |
| Safety slip-rate | 1.9% | 0.8% | -1.1 | â‰¤0.1% |
| Demo p95 latency | 9.3s | 8.4s | -0.9 | â‰¤8s |
| Test coverage | 68% | 85% | +17 | â‰¥85% |

### **Automated Delta Calculation**
```python
def calculate_metrics_delta(baseline_file: str, current_file: str) -> dict:
    """Calculate metric deltas for progress tracking"""
    try:
        baseline = load_metrics(baseline_file)
        current = load_metrics(current_file)
        return {k: current[k] - baseline[k] for k in baseline.keys()}
    except FileNotFoundError:
        return {"error": "Metrics files not found"}
```

---

## ğŸš¨ **Risk Mitigation**

### **Technical Risks**
| Risk | Probability | Impact | Mitigation | Owner |
|------|-------------|--------|------------|-------|
| Import path fixes fail | Low | High | Test with multiple files and fallback paths | Platform |
| Safety targets unmet | Medium | High | 10 high-precision rules + threshold tuning | ML |
| Stub drift regression | Low | Medium | CI guardrails + nightly real API tests | Platform |
| Performance targets unmet | Medium | Medium | Best-of-n optimization + caching | ML |

### **Contingency Plans**
- **If Priority 1 fails**: Fall back to stub mode for demo
- **If Priority 2 fails**: Accept higher slip rate temporarily
- **If Priority 3 fails**: Manual testing until CI is fixed

---

## ğŸ“Š **Monitoring & Alerts**

### **GitHub Actions Integration**
The existing `.github/workflows/ci.yml` already includes:
- âœ… API key integration via `${{ secrets.CLAUDE_API_KEY }}`
- âœ… Test execution with proper environment
- âœ… Artifact upload for results

### **Success/Fail Notifications**
```yaml
# .github/workflows/evidence.yml (if needed)
- name: Notify on failure
  if: failure()
  run: |
    echo "ğŸš¨ Evidence run failed: ${{ github.run_id }}"
    echo "Investigate: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

- name: Notify on success
  if: success()
  run: |
    echo "âœ… Evidence run completed: ${{ github.run_id }}"
    echo "View results: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
```

---

## ğŸ’° **Cost Analysis**

### **API Costs (Estimated)**
- **HumanEval-164**: ~$50-100 (with caching)
- **Harm suite (500 tests)**: ~$20-40
- **Development/testing**: ~$30-60
- **Total estimated cost**: $100-200

### **Cost Optimization**
- âœ… Progressive sampling reduces redundant calls
- âœ… Caching prevents duplicate executions
- âœ… Early stopping on success criteria
- âœ… Stub mode for development/testing

---

## ğŸ¬ **Demo Strategy**

### **Primary Demo Flow**
1. **Introduction** (5s): Oversight curriculum overview
2. **Core Pipeline** (8s): Execution demonstration
3. **Safety Features** (5s): Harm detection showcase
4. **Results** (3s): Performance metrics
5. **Q&A** (4s): Technical discussion

### **Fallback Options**
- ğŸ“¹ **Screen recording** of successful runs
- ğŸ“ **Demo script** with timing and narration
- ğŸ“Š **Technical metadata** for Q&A backup
- ğŸ”„ **Execution logs** for detailed analysis

---

## ğŸš€ **Execution Timeline**

### **Phase 1: Core Fixes (2 hours)**
```bash
# Priority 1 implementation
git checkout -b fix/core-pipeline-validation

# Fix import paths in test files
# Update test_humaneval.py to use oversight/ instead of src/
# Test oversight module execution
# Validate results with make targets
```

### **Phase 2: Safety Improvements (2-3 hours)**
```bash
# Priority 2 implementation
git checkout -b fix/safety-improvements

# Run harm suite tests
make harm-suite

# Implement safety rules
# (Manual implementation of 10 high-precision patterns)

# Validate improvements
python run_harm_suite.py --limit 50 --verbose
```

### **Phase 3: Testing & Guardrails (1 hour)**
```bash
# Priority 3 implementation
git checkout -b fix/testing-guardrails

# Add CI guardrails
# Implement progress tracking
# Test with make targets
make dev-pytest
make test
make check
```

---

## ğŸ¯ **Success Validation**

### **Immediate Success Indicators**
- âœ… Core pipeline executes without errors
- âœ… HumanEval smoke test passes
- âœ… Safety slip rate â‰¤ 0.1%
- âœ… Test coverage â‰¥ 85%

### **Comprehensive Success Criteria**
- âœ… **Primary target**: pass@1 â‰¥ 0.6
- âœ… **Fallback target**: pass@1 â‰¥ 0.45 OR uplift â‰¥ 8pp
- âœ… **Safety target**: â‰¤ 0.1% harmful responses
- âœ… **Performance target**: Demo latency â‰¤ 8s
- âœ… **Quality target**: Test coverage â‰¥ 85%

---

## ğŸ“‹ **Next Steps**

### **Immediate Actions**
1. **Execute Priority 1** fixes to validate core pipeline
2. **Monitor execution** and collect baseline metrics
3. **Implement Priority 2** safety improvements
4. **Add Priority 3** testing guardrails

### **Success Tracking**
- **Daily progress** updates via automated metrics
- **Weekly reviews** of success probability
- **Continuous monitoring** of key performance indicators

---

## ğŸ› ï¸ **File Fixes Required**

### **Import Path Fixes**
The following files need import path corrections:

1. **test_humaneval.py**: Change `src/` to `oversight/`
2. **run_harm_suite.py**: Change `src/` to `oversight/`
3. **scripts/smoke_test.py**: Already fixed in previous session

### **Command Interface Updates**
- âœ… `python -m oversight run --mode demo` (works correctly)
- âœ… `make quick-humaneval` (exists and works)
- âœ… `make harm-suite` (exists and works)
- âœ… `make dev-pytest` (exists and works)

### **Error Handling**
- Add fallback for missing API key
- Add graceful degradation for import failures
- Add stub mode for development testing

---

**ğŸ¯ Ready for execution with 45% success probability and clear path to improvement!**
